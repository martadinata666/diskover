---

# project information
project_name: diskover
project_url: "https://github.com/shirosaidev/diskover"
project_logo: "https://raw.githubusercontent.com/shirosaidev/diskover/master/docs/diskover.png"
project_blurb: "[{{ project_name }}]({{ project_url }}) is a file system crawler and disk space usage software that uses Elasticsearch to index and manage data across heterogeneous storage systems."
project_lsio_github_repo_url: "https://github.com/linuxserver/docker-{{ project_name }}"

# supported architectures
available_architectures:
  - { arch: "{{ arch_x86_64 }}", tag: "amd64-latest"}
  - { arch: "{{ arch_arm64 }}", tag: "arm64v8-latest"}
  - { arch: "{{ arch_armhf }}", tag: "arm32v7-latest"}

# container parameters
common_param_env_vars_enabled: true #PGID, PUID, etc
param_container_name: "{{ project_name }}"
param_usage_include_vols: true
param_volumes:
  - { vol_path: "/config", vol_host_path: "/path/to/{{ project_name }}/config", desc: "Persistent config files" }
  - { vol_path: "/data", vol_host_path: "/path/to/{{ project_name }}/data", desc: "Default mount point to crawl" }
param_usage_include_ports: true
param_ports:
  - { external_port: "80", internal_port: "80", port_desc: "diskover Web UI" }
  - { external_port: "9181", internal_port: "9181", port_desc: "rq-dashboard web UI" }
  - { external_port: "9999", internal_port: "9999", port_desc: "diskover socket server" }
param_usage_include_env: true
param_env_vars:
  - { env_var: "TZ", env_value: "Europe/London", desc: "Specify a timezone to use EG Europe/London"}
  - { env_var: "REDIS_HOST", env_value: "redis", desc: "Redis host (optional)"}
  - { env_var: "REDIS_PORT", env_value: "6379", desc: "Redis port (optional)"}
  - { env_var: "ES_HOST", env_value: "elasticsearch", desc: "ElasticSearch host (optional)"}
  - { env_var: "ES_PORT", env_value: "9200", desc: "ElasticSearch port (optional)"}
  - { env_var: "ES_USER", env_value: "elastic", desc: "ElasticSearch username (optional)"}
  - { env_var: "ES_PASS", env_value: "changeme", desc: "ElasticSearch password (optional)"}
  - { env_var: "INDEX_NAME", env_value: "diskover-", desc: "Index name prefix (optional)"}
  - { env_var: "DISKOVER_OPTS", env_value: "", desc: "Optional arguments to pass to the diskover crawler (optional)"}
  - { env_var: "WORKER_OPTS", env_value: "", desc: "Optional argumens to pass to the diskover bots launcher (optional)"}
  - { env_var: "RUN_ON_START", env_value: "true", desc: "Initiate a crawl every time the container is started (optional)"}
  - { env_var: "USE_CRON", env_value: "true", desc: "Run a crawl on as a cron job (optional)"}

custom_compose: |
  version: '2'
  services:
    diskover:
      image: linuxserver/diskover
      container_name: diskover
      environment:
        - PUID=1000
        - PGID=1000
        - TZ=Europe/London
        - REDIS_HOST=redis
        - REDIS_PORT=6379
        - ES_HOST=elasticsearch
        - ES_PORT=9200
        - ES_USER=elastic
        - ES_PASS=changeme
        - RUN_ON_START=true
        - USE_CRON=true
      volumes:
        - /path/to/diskover/config:/config
        - /path/to/diskover/data:/data
      ports:
        - 80:80
        - 9181:9181
        - 9999:9999
      mem_limit: 4096m
      restart: unless-stopped
      depends_on:
        - elasticsearch
        - redis
    elasticsearch:
      container_name: elasticsearch
      image: docker.elastic.co/elasticsearch/elasticsearch:5.6.9
      volumes:
        - ${DOCKER_HOME}/elasticsearch/data:/usr/share/elasticsearch/data
      environment:
        - bootstrap.memory_lock=true
        - "ES_JAVA_OPTS=-Xms2048m -Xmx2048m"
      ulimits:
        memlock:
          soft: -1
          hard: -1
    redis:
      container_name: redis
      image: redis:alpine
      volumes:
        - ${HOME}/docker/redis:/data

# application setup block
app_setup_block_enabled: true
app_setup_block: |
  Once running the URL will be `http://<host-ip>/` initial application spinup will take some time so please reload if you get an empty response. We highly reccomend using Docker compose for this image as it includes multiple database backends to link into.
  If you are looking to mount the elasticsearch and redis data to your host machine for access neither of them currently support setting a custom UID or GID they will run by default as:

  - Redis - UID=999 GID=999
  - Elasticsearch - UID=1000 GID=1000

  ElasticSearch also requires a sysctl setting on the host machine to run properly. Running `sysctl -w vm.max_map_count=262144` will solve this issue. To make this setting persistent through reboots, set this value in `/etc/sysctl.conf`.

  If you simply want the application to work you can mount these to folders with 0777 permissions, otherwise you will need to create these users host level and set the folder ownership properly.

  By default this compose example is pointed to a single directory and the UID and GID you pass to the diskover container needs to match that folders ownership. If these are shared folders with many owners the indexing will likely fail.

  For specific questions or help setting up diskover in your environment please refer to the project's Github page [{{ project_name|capitalize }}]({{ project_url }}).

# changelog
changelogs:
  - { date: "19.11.20:", desc: "Fix pip packages." }
  - { date: "19.12.19:", desc: "Rebasing to alpine 3.11." }
  - { date: "28.06.19:", desc: "Rebasing to alpine 3.10." }
  - { date: "12.04.19:", desc: "Rebase to Alpine 3.9." }
  - { date: "23.03.19:", desc: "Switching to new Base images, shift to arm32v7 tag." }
  - { date: "01.11.18:", desc: "Initial Release." }
